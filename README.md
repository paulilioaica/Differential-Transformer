# Transformer Plain

🤖🔮🔥

## Overview
This is a PyTorch project that implements a plain transformer architecture for self-supervised prediction, which is at the core of LLMs. This project aims to provide a simple and efficient implementation of the transformer model, allowing users to train their own models for various tasks.

## Features

✨ Easy-to-use: The project provides a straightforward setup and training loop for self-supervised prediction.

🧠 Transformer Architecture: The project implements the popular transformer architecture, which has shown great success in various natural language processing and computer vision tasks.

🔀 Self-Supervised Prediction: The training loop is designed to support self-supervised prediction, enabling the model to learn from unlabeled data.

## Setup

To get started with Transformer Plain, follow these steps:

1. Clone the repository:

    ```shell
    git clone https://github.com/paulilioaica/Vanilla-Pytorch-Transformer
    ```

2. Install the required dependencies:

    ```shell
    pip install -r requirements.txt
    ```

## Usage

1. Dataset: Make sure you have a dataset suitable for self-supervised prediction from Huggingface (or use the AG-NEWS one). Simply pass the `dataset_name` for training on your dataset of choice.

2. Configure the training parameters: Adjust the hyperparameters by passing your own arguments.

3. Train the model: Run the training script to start the self-supervised prediction training loop.

4. Evaluate the model: Use the trained model to make predictions on your test dataset and evaluate its performance.


## License

This project is licensed under the MIT License. 